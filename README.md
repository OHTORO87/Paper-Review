# Paper-Review
 - Paper Review about Speech Recognition · NLP · Speech Synthesis  
 
   
|Year|Paper|Links|
|:---:|:---:|:---:|
|`2006/06`|Connectionist Temporal Classification: Labelling UnsegmentedSequence Data with Recurrent Neural Networks|[Paper](https://www.cs.toronto.edu/~graves/icml_2006.pdf) [Review](https://github.com/hasangchun/Paper-Review/blob/main/Review/Connectionist%20Temporal%20Classification.pdf)|
|`2015/06`|Attention-Based Models for Speech Recognition|[Paper](https://arxiv.org/pdf/1506.07503.pdf) [Review](https://github.com/hasangchun/Paper-Review/blob/main/Review/Attention-Based%20Models%20for%20Speech%20Recognition.md)|
|`2015/08`|Listen, Attend and Spell|[Paper](https://arxiv.org/pdf/1508.01211.pdf) [Review](https://github.com/hasangchun/Paper-Review/blob/main/Review/Listen%2C%20Attend%20and%20Spell.pdf)|
|`2016/09`|Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning|[Paper](https://arxiv.org/pdf/1609.06773.pdf) [Review](https://github.com/hasangchun/Paper-Review/blob/main/Review/JOINT%20CTC-ATTENTION%20BASED%20END-TO-END%20SPEECH%20RECOGNITION.md)|
|`2017/07`|Attention Is All You Need|[Paper](https://arxiv.org/pdf/1706.03762.pdf) [Review](https://github.com/hasangchun/Paper-Review/blob/main/Review/Attention%20Is%20All%20You%20Need.md)|
|`2018/10`|BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding|[Paper](https://arxiv.org/pdf/1810.04805.pdf) [Review](https://github.com/hasangchun/Paper-Review/blob/main/Review/BERT_%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.md)|
|`2019/04`|SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition|[Paper](https://arxiv.org/pdf/1904.08779.pdf) [Review](https://github.com/hasangchun/Paper-Review/blob/main/Review/SpecAugment_%20A%20Simple%20Data%20Augmentation%20Method%20for%20Automatic%20Speech%20Recognition.md)|

